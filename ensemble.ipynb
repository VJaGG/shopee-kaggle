{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "sys.path.append(\"/home/data_normal/abiz/wuzhiqiang/wzq/shopee/code_v3/pytorch-image-models\")\n",
    "import timm\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import albumentations as A\n",
    "import torch.utils.data as data\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoConfig, AutoModel\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_arch = \"bert-base-multilingual-uncased\"\n",
    "root = \"/home/data_normal/abiz/wuzhiqiang/wzq/data/shopee-product-matching/test.csv\"\n",
    "test_path = \"/home/data_normal/abiz/wuzhiqiang/wzq/data/shopee-product-matching/test_images\"\n",
    "image_initial_checkpoint = \"/home/data_normal/abiz/wuzhiqiang/wzq/shopee/code_v3/result/image/00062000_model.pth\"\n",
    "text_initial_checkpoint = \"/home/data_normal/abiz/wuzhiqiang/wzq/shopee/code_v3/result/text/00052000_model.pth\"\n",
    "df = pd.read_csv(root)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_2255846744</td>\n",
       "      <td>0006c8e5462ae52167402bac1c2e916e.jpg</td>\n",
       "      <td>ecc292392dc7687a</td>\n",
       "      <td>Edufuntoys - CHARACTER PHONE ada lampu dan mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_3588702337</td>\n",
       "      <td>0007585c4d0f932859339129f709bfdc.jpg</td>\n",
       "      <td>e9968f60d2699e2c</td>\n",
       "      <td>(Beli 1 Free Spatula) Masker Komedo | Blackhea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_4015706929</td>\n",
       "      <td>0008377d3662e83ef44e1881af38b879.jpg</td>\n",
       "      <td>ba81c17e3581cabe</td>\n",
       "      <td>READY Lemonilo Mie instant sehat kuah dan goreng</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        posting_id                                 image       image_phash  \\\n",
       "0  test_2255846744  0006c8e5462ae52167402bac1c2e916e.jpg  ecc292392dc7687a   \n",
       "1  test_3588702337  0007585c4d0f932859339129f709bfdc.jpg  e9968f60d2699e2c   \n",
       "2  test_4015706929  0008377d3662e83ef44e1881af38b879.jpg  ba81c17e3581cabe   \n",
       "\n",
       "                                               title  \n",
       "0  Edufuntoys - CHARACTER PHONE ada lampu dan mus...  \n",
       "1  (Beli 1 Free Spatula) Masker Komedo | Blackhea...  \n",
       "2   READY Lemonilo Mie instant sehat kuah dan goreng  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_threshold = 50\n",
    "text_threshold = 70\n",
    "batch_size = 4\n",
    "width, height = 640, 640\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "null_augment = A.Compose(\n",
    "    [A.Resize(width, height)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    batch_size = len(batch)\n",
    "    image = []\n",
    "    index = []\n",
    "    input_ids = []\n",
    "    token_type_ids = []\n",
    "    attention_mask = []\n",
    "    for r in batch:\n",
    "        image.append((r['image'] / 255.0 - mean) / std)\n",
    "        index.append(r['index'])\n",
    "        input_ids.append(r['items']['input_ids'])\n",
    "        token_type_ids.append(r['items']['token_type_ids'])\n",
    "        attention_mask.append(r['items']['attention_mask'])\n",
    "    \n",
    "    image = np.stack(image)\n",
    "    input_ids = torch.stack(input_ids)\n",
    "    token_type_ids = torch.stack(token_type_ids)\n",
    "    attention_mask = torch.stack(attention_mask)\n",
    "\n",
    "    image = image.transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image).contiguous().float()\n",
    "\n",
    "    return {\n",
    "        'index': index,\n",
    "        'image': image,\n",
    "        'input_ids': input_ids,\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShopeeDataset(data.Dataset):\n",
    "    def __init__(self, df, augment=null_augment, bert_model_arch=bert_model_arch):\n",
    "        self.df = df\n",
    "        self.augment = augment\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model_arch)\n",
    "        texts = df['title'].fillna(\"NaN\").tolist()  # 所有标题生成的文本列表\n",
    "        self.encodings = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        posting_info = self.df.iloc[index]\n",
    "        title = posting_info.title\n",
    "        image_name = posting_info.image\n",
    "        image_path = os.path.join(test_path, image_name)\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = image.astype(np.float32)\n",
    "\n",
    "        items = {k: torch.tensor(v[index]) for k, v in self.encodings.items()}\n",
    "        \n",
    "        if self.augment:\n",
    "            augmented = self.augment(image=image)\n",
    "            image = augmented['image']\n",
    "\n",
    "        r = {\n",
    "            'index': index,\n",
    "            'image': image,\n",
    "            'items': items\n",
    "        }\n",
    "        return r\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __str__(self):\n",
    "        string = ''\n",
    "        string += '\\tlen     = %d\\n' % len(self)\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcMarginProductMargin(nn.Module):\n",
    "    \"\"\"Implement of large margin arc distance: :\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        s: norm of input feature\n",
    "        m: margin\n",
    "        cos(theta + m)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        s: float = 30.0,\n",
    "        easy_margin: bool = False,\n",
    "        smoothing: float = 0.0,\n",
    "    ):\n",
    "        super(ArcMarginProductMargin, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "\n",
    "    def forward(self, inputs, labels, margin=0.5):\n",
    "        cos_m = math.cos(margin)\n",
    "        sin_m = math.sin(margin)\n",
    "        th = math.cos(math.pi - margin)\n",
    "        mm = math.sin(math.pi - margin) * margin\n",
    "\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        cosine = F.linear(F.normalize(inputs), F.normalize(self.weight)).float()\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * cos_m - sine * sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > th, phi, cosine - mm)\n",
    "\n",
    "        # --------------------------- convert label to one-hot ---------------------------\n",
    "        one_hot = torch.zeros(cosine.size(), device=labels.device)\n",
    "        one_hot.scatter_(1, labels.view(-1, 1).long(), 1)\n",
    "        if self.smoothing > 0:\n",
    "            one_hot = (\n",
    "                1 - self.smoothing\n",
    "            ) * one_hot + self.smoothing / self.out_features\n",
    "\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NFNetMargin(nn.Module):\n",
    "    def __init__(self, arch=\"eca_nfnet_l1\", dim=1792, num_classes=11014, pretrained=False):\n",
    "        super(NFNetMargin, self).__init__()\n",
    "\n",
    "        self.backbone = timm.create_model(arch, pretrained=pretrained)\n",
    "        final_in_features = self.backbone.head.fc.in_features\n",
    "        self.backbone.head.global_pool = nn.Identity()\n",
    "        self.backbone.head.fc = nn.Identity()\n",
    "        self.conv = nn.Conv2d(final_in_features, dim, kernel_size=3, stride=1)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.bn = nn.BatchNorm1d(dim)\n",
    "        \n",
    "        self.margin = ArcMarginProductMargin(in_features=dim,\n",
    "                                             out_features=num_classes)\n",
    "        self.__init_params()\n",
    "    \n",
    "    def __init_params(self):\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    def forward(self, x, labels=None, margin=0.5):\n",
    "        feature = self.backbone(x)\n",
    "        feature = self.conv(feature)\n",
    "        feature = self.silu(feature)\n",
    "        feature = self.pooling(feature).view(x.size(0), -1)\n",
    "        feature = self.bn(feature)  # (4, 2048)\n",
    "        if labels is not None:\n",
    "            return self.margin(feature, labels, margin)\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTMargin(nn.Module):\n",
    "        \n",
    "    def __init__(self, arch='bert-base-multilingual-uncased',\n",
    "                    hidden_size=768, dim=1024, num_classes=11014):\n",
    "        \n",
    "        super(BERTMargin, self).__init__()\n",
    "#         path = \"../input/huggingface-bert/bert-base-multilingual-uncased\"\n",
    "        config = AutoConfig.from_pretrained(arch,\n",
    "                                            output_hidden_states=True)\n",
    "        self.bert_model = AutoModel.from_pretrained(\n",
    "            arch,\n",
    "            cache_dir=None,\n",
    "            config=config,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc = nn.Linear(\n",
    "            hidden_size, \n",
    "            dim\n",
    "        )\n",
    "        self.bn = nn.BatchNorm1d(dim)\n",
    "        self.margin = ArcMarginProductMargin(in_features=dim,\n",
    "                                            out_features=num_classes)\n",
    "        self._init_params()\n",
    "\n",
    "\n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids, attentions_mask, labels=None, margin=0.8):\n",
    "        # input_ids: (None, 128)\n",
    "        # attentions_mask: (None, 128)\n",
    "        output = self.bert_model(input_ids=input_ids, attention_mask=attentions_mask)\n",
    "        hs = output.hidden_states\n",
    "        hs_idxs = [-1, -2, -3, -4]\n",
    "        seq_output = torch.stack([hs[idx] for idx in hs_idxs]).mean(dim=0)\n",
    "        avg_output = torch.sum(\n",
    "            seq_output * attentions_mask.unsqueeze(-1),\n",
    "            dim=1,\n",
    "            keepdim=False\n",
    "        )\n",
    "        avg_output = avg_output / torch.sum(attentions_mask, dim=-1, keepdim=True)\n",
    "        x = avg_output\n",
    "\n",
    "        out = self.fc(x)\n",
    "        out = self.bn(out)\n",
    "        if labels is not None:\n",
    "            return self.margin(out, labels, margin)\n",
    "        return out, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_feat(net, valid_loader):\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        valid_num = 0\n",
    "        for t, batch in enumerate(valid_loader):\n",
    "            index = batch['index']\n",
    "            image = batch['image'].cuda()\n",
    "            feat = net(image)\n",
    "            features += [feat.detach().cpu()]\n",
    "            valid_num += len(index)\n",
    "        assert (valid_num == len(valid_loader.dataset))\n",
    "    features = torch.cat(features).cpu().numpy()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_feat(net, valid_loader):\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        valid_num = 0\n",
    "        for t, batch in enumerate(valid_loader):\n",
    "            index = batch['index']\n",
    "            input_ids = batch['input_ids'].cuda()\n",
    "            token_type_ids = batch['token_type_ids'].cuda()\n",
    "            attention_mask = batch['attention_mask'].cuda()\n",
    "            feat, _ = net(input_ids, attention_mask)\n",
    "            features += [feat.detach().cpu()]\n",
    "            valid_num += len(index)\n",
    "        assert (valid_num == len(valid_loader.dataset))\n",
    "    features = torch.cat(features).cpu().numpy()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(posting_ids, threshold, features, n_batches, min_indices=1):\n",
    "    assert len(posting_ids) == len(features)\n",
    "    sim_threshold = threshold / 100\n",
    "    y_pred = []\n",
    "    n_rows = features.shape[0]\n",
    "    bs = n_rows // n_batches\n",
    "    batches = []\n",
    "    for i in range(n_batches):\n",
    "        left = bs * i\n",
    "        right = bs * (i + 1)\n",
    "        if i == n_batches - 1:\n",
    "            right = n_rows\n",
    "        batches.append(features[left: right, :])\n",
    "    for batch in batches:\n",
    "        dot_product = batch @ features.T\n",
    "        selection = dot_product > sim_threshold\n",
    "        for j in range(len(selection)):\n",
    "            IDX = selection[j]  # 阈值之内的相似图片\n",
    "            if np.sum(IDX) < min_indices:\n",
    "                IDX = np.argsort(dot_product[j])[-min_indices:]\n",
    "            y_pred.append(posting_ids[IDX].tolist())\n",
    "    assert len(y_pred) == len(posting_ids)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ShopeeDataset(df)\n",
    "test_dataloader = data.DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgnet = NFNetMargin(num_classes=8811)\n",
    "txtnet = BERTMargin(num_classes=8811)\n",
    "imgnet.to(device)\n",
    "txtnet.to(device)\n",
    "\n",
    "state_dict = torch.load(image_initial_checkpoint, map_location=lambda storage, loc: storage)['state_dict']\n",
    "imgnet.load_state_dict(state_dict,strict=False)\n",
    "imgnet.eval()\n",
    "del state_dict\n",
    "\n",
    "state_dict = torch.load(text_initial_checkpoint, map_location=lambda storage, loc: storage)['state_dict']\n",
    "txtnet.load_state_dict(state_dict,strict=False)\n",
    "txtnet.eval()\n",
    "del state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = extract_image_feat(imgnet, test_dataloader)\n",
    "text_features = extract_text_feat(txtnet, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_feature = np.hstack((image_features, text_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = F.normalize(torch.from_numpy(image_features)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = F.normalize(torch.from_numpy(text_features)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_feature = F.normalize(torch.from_numpy(concat_feature)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "posting_ids = df['posting_id']\n",
    "y_image_pred = find_matches(posting_ids, image_threshold, image_features, n_batches=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_text_pred =  find_matches(posting_ids, text_threshold, text_features, n_batches=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_pred =  find_matches(posting_ids, 50, concat_feature, n_batches=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [list(set(i + j + k)) for i, j, k in zip(y_image_pred, y_text_pred, concat_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['matches'] = y_pred\n",
    "df['matches'] = df['matches'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['posting_id','matches']].to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_2255846744</td>\n",
       "      <td>test_2255846744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_3588702337</td>\n",
       "      <td>test_3588702337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_4015706929</td>\n",
       "      <td>test_4015706929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        posting_id          matches\n",
       "0  test_2255846744  test_2255846744\n",
       "1  test_3588702337  test_3588702337\n",
       "2  test_4015706929  test_4015706929"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('submission.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
